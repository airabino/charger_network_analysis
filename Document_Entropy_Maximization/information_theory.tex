\section{Information Theory}

The core concept of information theory is the quantification of the informational value of a given system state. The degree to which a state provides information is the degree to which that state is unexpected. As an example, a lottery draw selects just one sequence from a massive set of possible sequences each one of which is equally likely to be the winner. Learning that a given sequence is not a winner provides little information concerning the winning sequence. As the draw progresses each number eliminates the vast majority of remaining possible sequences making all remaining possible winners much more likely to win. Most lottery players will be eliminated by a given number draw and yet the announcement will state the number selected instead of the numbers eliminated because it is a much more efficient way to provide the same information.

The information content of an event $E$ in inversely proportional to the likelihood of event $E$. The information content of event $E$ is defined as

\begin{equation}
	I(E) = log\left(\frac{1}{p(E)}\right) \label{eq:information_content}
\end{equation}

where $p(E)$ is the probability of event $E$ and the base of the logarithm corresponds to the size of the set of possible values for $E$ and is often 2, $e$, or 10. Entropy ($S$) is the expectation of information content for a random trial. Entropy is defined as

\begin{equation}
	S = \sum_{E \in \hat{E}}p(E)I(E) = \sum_{i = 1}^{N}p(E)log\left(\frac{1}{p(E)}\right) \label{eq:entropy}
\end{equation}

where $\hat{E}$ is the set of possible events. Let $A$ be a sequence of 2 coin flips where $p(h) = 1 - p(t) = 0.5$. The possible sequences for $A$ are $hh$, $ht$, $th$, and $tt$ all with a probability of $0.25$. Using base 2, the entropy for $A$ is 2 bits. In other words, prior to the two coin flips one has no information on what sequence will come up. Similarly, the information content for any possible sequence in $A$ is 2 bits meaning that, after two flips, one is completely certain what sequence has come up. If, however, $p(h) = 1 - p(t) = 0.75$ then $p(hh) = 0.5625$, $p(ht) = p(th) = 0.375$, and $p(tt) = 0.0625$. In this case $I(hh) = 0.83$, $I(ht) = I(th) = 1.42$, and $I(tt) = 4.0$ and the entropy of $A$ is $0.864$. Because of the difference in odds, one can be more certain about the outcome of the latter scenario than of the former scenario before any flips have taken place. Similarly, the uncertainty of two dice rolls is greater than that of two coin flips even if both are "fair" because there are more possibilities for each dice roll.

Selection of the logarithmic base is somewhat arbitrary and, as long as consistently applied, will not effect the order of entropy among sets. However, there is physical meaning to entropy and the logarithmic base generally reflects what type of problem is being solved. In communications the base is usually 2 and the units of entropy are bits. The physical interpretation of this is that in order to attain relative certainty about the content of a message a certain number of bits are required. English words are largely composed of common sequences of letters and thus, if one knows that a sequence of ASCII characters represent English text then the entropy of the sequence is much lower than if one has to assume the sequence is random. English sentences also mostly follow a common structure described by subject-verb-object word ordering and preceding articles and adjectives which allows for sequence-trained large language models to generate highly plausible text sequences. In statistical thermodynamics Shannon Entropy is computed using natural logarithms where the unit of entropy is Joules per Kelvin, the same units as heat capacity, allowing entropy to be used in thermodynamic equations.


